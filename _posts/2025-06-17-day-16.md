---
layout: post
title: "Day 16 â€“ Experimenting with Optimizers and Hyperparameter Tuning with Optuna"
date: 2025-06-17
author: Yusrat Miah
permalink: /day16.html
tags: ["DenseNet-121", "Optuna", "Adam Optimizer", "Lion Optimizer"]

what_i_learned: |
  After having difficulties with utlizing the GPU that is in my workstation PC, I communicated the iisue with my graduate mentor and was able to use another available workstation in the lab to run my Densenet-121 model. The model I ran locally on Vscode and through the installation of the necessary packages, took approximately 3 hours to run 10 epochs. It was surprising to see that it still took a lot of time to run the model. While waiting for the models to finish running, I enlightened myself on concepts such as hyperparameter tuning and optimizers, which are all important concepts to explore to enhance the ultimate ensemble model for our research.

blockers: |
  The day did start with us trying to activate the GPU on two of the workstations, but that was not a success. Instead, we ended up running the two different models on the PC that our graduate mentor opened access to today.

reflection: |
  Although we started the day with some setbacks (not being able to configure our work stations), I would say my day significantly improved once we were able to run the models and get some results. I specifically liked learning about the new optimizer named LION that apparently has show to beat the performance of the Adam optimizer. I also enjoyed writing some code for using Optuna to see which hyperparemters would be ideal for DenseNet-121. However, I learned through this that running an Optuna study takes a while (upwards to multiple hours to multiple days). My goal for tomorrow is to increase the epochs when running the models and going back to some reference notes that I made for hyperparamters. 


---
