---
layout: post
title: "Day 21 â€“ Cohort Meeting, GPU Optimization, Midsummer Symposium Team Member Assignments"
date: 2025-06-24
author: Yusrat Miah
permalink: /day21.html
tags: ["Cuda", "Anaconda", "DenseNet-121", "Machine Learning", "Deep Learning"]

what_i_learned: |
  The day began in the Graves Business Building with a Cohort meeting with Dr. Mack, where she went over the expectations of the program and the upcoming group presentations that will take place on Friday, June 27th. In particular, Dr. Mack explained the importance of preparing for the presentation since it will be a good way to gauge on what still needs to be accomplished for the rest of the summer and also will be a good way to show case what each group has accomplished so far. I liked how she emphasized that each group should practice their presenations to ensure that timing is right (somewhere between 15-20 minutes long). After the meeting ended, I went back to my lab and continued to fine-tune my DenseNet-121 model by running some models with a larger epoch number and with different hyperparameters. While running the first iteration of the model today, I noticed that per epoch took around 30-45 mins, which was still signifcantly slow even though I was connected locally on to my work station computer that has an NVIDIA GPU. I then checked my setup in my Anaconda environment and realized that my Juoyter notebook local host was not detecting the GPU. This led me to debug the environment install process by first checking the python and tensorflow versions, and then redownloading/installing consulting the medium post titled, "Install CUDA, cuDNN in conda virtual environment and setup GPU support using Tensorflow." From there, I did the following: 1) Installing CUDA, cuDNN with conda-forge 2) Installing Tensorflow GPU and lastly, 3) installing a version numpy that is LESS than 2.0.0. Once I completed these steps, I was able to utlized the GPU, and it was evident that the GPU was in use since the time per epoch reduced to somewhere betweeen 2 to 15 minutes! I was also able to help my group members who were encountering the same issues. Lastly, we virtually met up with our graduate mentor to discuss what slides each of us will be working on for the presentation on Friday. I jotted down detail notes during the meeting and will organize the notes for reference. I also setup another model with 64 epochs, Adam optimzier, sigmoid activation, and batch size of 64 that will run over night. Towards the end, I started to read the paper titled, "ntelligent Deep Convolutional Neural Network Based Object Detection Model for Visually Challenged People."
blockers: |
  One blocker I encountered was ensuring proper GPU detection and activation within my Conda environment, which initially slowed down model training significantly
reflection: |
  Today was a productive blend of collaboration, troubleshooting, and model refinement. After gaining valuable insights during our cohort meeting with Dr. Mack about presentation expectations, I dove back into optimizing my DenseNet-121 model. Identifying a GPU detection issue in my local environment led me down a successful path of debugging and reconfiguring TensorFlow and CUDA in Conda, which significantly improved training speed. I was also able to assist my peers with the same problem, which felt rewarding. The day wrapped up with team planning and some evening reading that gave me new inspiration for our project's direction.

---
