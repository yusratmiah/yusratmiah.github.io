---
layout: post
title: "Day 22 – Writing a New Codebase and Running More Models"
date: 2025-06-25
author: Yusrat Miah
permalink: /day22.html
tags: ["DenseNet-121 Architecture", "Statistics", "Activation Functions"]

what_i_learned: |
  Today I learned a lot by diving into both hands-on experimentation and deeper theoretical insights. I started off by documenting two overnight training runs with different configurations, which helped me understand how changes like batch size, activation functions, and training length affect performance. Rewriting the DenseNet-121 codebase based on a Kaggle notebook was especially rewarding—the new version performed significantly better, even after just five epochs. I also learned why softmax is the go-to choice for multi-class classification: it transforms model outputs into a probability distribution, making the predictions easier to interpret and compare. Reading the review paper on activation functions reinforced that understanding and gave me a broader perspective on how different functions behave in neural networks.
  
blockers: |
  A blocker was the underperformance of the original DenseNet-121 codebase, which required me to troubleshoot and rewrite a new version.
  
reflection: |
  Reflecting on today, I pushed both the technical and conceptual sides of my project. Reworking the DenseNet-121 code from scratch and seeing immediate accuracy showed me the value of taking initiative to explore alternatives. Understanding why softmax is the go-to activation function for classification deepened my grasp of neural network architecture and the logic behind output layer design. Organizing meeting notes and reading the review paper helped tie everything together. Overall, it was a day of clarity, experimentation, and meaningful progress.

---
