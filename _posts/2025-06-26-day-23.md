---
layout: post
title: "Day 23 – Continuation of Fine-Tuning Models and Discussing Results"
date: 2025-06-26
author: Yusrat Miah
permalink: /day23.html
tags: ["DenseNet-121 Architecture", "Ensemble Models", "Vscode"]

what_i_learned: |
  I began my day by exploring ways to improve my existing model. I specifically watched a Stanford Engineering School lecture titled "Training Neural Networks II." This lecture helped clarify essential concepts related to hyperparameter tuning, particularly how regularization can mitigate overfitting—where a model performs well on training data but poorly on unseen data. I also gained a clearer perspective on a type of regularization called dropout, which randomly sets a neuron's activation value to zero during forward passes. Furthermore, these concepts align with transfer learning, where, when working with larger datasets, it’s beneficial to initially train the first few layers and then fine-tune the lower layers using a reduced learning rate (typically one-tenth of the original rate). Later in the day, I collaborated with my teammate Michelle to apply the AdamW optimizer and early stopping to my model. Through this, I learned how to use the tf.keras.callbacks.EarlyStopping() function and how setting the mode parameter to 'min' halts the training process once the monitored metric stops improving, based on the interval defined by the patience parameter.
  
blockers: |
  A blocker 
  
reflection: |
    

---
