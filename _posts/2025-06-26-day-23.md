---
layout: post
title: "Day 23 â€“ Continuation of Fine-Tuning Models and Discussing Results"
date: 2025-06-26
author: Yusrat Miah
permalink: /day23.html
tags: ["DenseNet-121 Architecture", "Ensemble Models", "Vscode"]

what_i_learned: |
  I began my day with enlightening myself about ways I can improve my exisiting model. I specfically watched the Stanford Engineering School's Lecture titled "Training Neural Networks II." By watching this lecture, it helped clarify some essential concepts for hyperparameter tuning since I learned about how regularization helps with mitigating the issues related to overfitting (performs too well on training data, poorly on unseen data). I also gained a good prespective on a type of regularization called dropout, which forward passes and randomly sets the neurons activation value to 0. Furthermore, these concepts align with transfer learning as when dealing with a bigger dataset, it helps with initially training the first couple of layers then transition to training further lower layers with lower learning rates (typically 1/10 of the original learning rate is a good starting point). 
  Later in the day, I worked closely with a my teammate Michelle to apply the AdamW optimizer and early-stopping on my model. Through this, I learned how to apply the tf.keras.callbacks.EarlyStopping() function and how setting the mode parameter to 'min' will hault the training process after it reaches a lower value during the checks of the specfic interval set at the patience level of the early stopping function. 
  
blockers: |
  A blocker 
  
reflection: |
    

---
