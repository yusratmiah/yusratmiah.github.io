---
layout: post
title: "Day 23 – Continuation of Fine-Tuning Models and Discussing Results"
date: 2025-06-26
author: Yusrat Miah
permalink: /day23.html
tags: ["DenseNet-121 Architecture", "Ensemble Models", "Vscode"]

what_i_learned: |
  I began my day by exploring ways to improve my existing model. I specifically watched a Stanford Engineering School lecture titled "Training Neural Networks II." This lecture helped clarify essential concepts related to hyperparameter tuning, particularly how regularization can mitigate overfitting—where a model performs well on training data but poorly on unseen data. I also gained a clearer perspective on a type of regularization called dropout, which randomly sets a neuron's activation value to zero during forward passes. Furthermore, these concepts align with transfer learning, where, when working with larger datasets, it’s beneficial to initially train the first few layers and then fine-tune the lower layers using a reduced learning rate (typically one-tenth of the original rate). Later in the day, I collaborated with my teammate Michelle to apply the AdamW optimizer and early stopping to my model. Through this, I learned how to use the tf.keras.callbacks.EarlyStopping() function and how setting the mode parameter to 'min' halts the training process once the monitored metric stops improving, based on the interval defined by the patience parameter.
  
blockers: |
  A blocker a faced was that I found it challenging to fully grasp how dropout works during training, especially how it balances model complexity and generalization without underfitting. 
  
reflection: |
  Today, I deepened my understanding of neural network optimization by exploring advanced techniques like regularization and dropout through a Stanford lecture. This helped clarify how these methods combat overfitting and improve model generalization. I also learned how transfer learning can be effectively applied by adjusting learning rates across layers. Collaborating with my teammate Michelle, I implemented the AdamW optimizer and early stopping, gaining hands-on experience with TensorFlow’s callback functions. Overall, this experience strengthened both my theoretical knowledge and practical skills in fine-tuning deep learning models.    

---
